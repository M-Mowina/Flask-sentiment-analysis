{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mohamed mowina\\anaconda3\\envs\\nngpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # type: ignore\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\mohamed\n",
      "[nltk_data]     mowina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the 'stopwords' resource\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#Tokenize using BERT tokenizer (optional, replace with your desired tokenizer)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = tf.keras.models.load_model('amazon_full_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "  # Lowercase text\n",
    "  text = text.lower()\n",
    "\n",
    "  # Remove numbers (optional)\n",
    "  text = re.sub('[0-9]+', '', text)  # Consider keeping numbers for specific domains\n",
    "\n",
    "  # Remove special characters and some punctuation\n",
    "  text = re.sub(r\"[^\\w\\s!@#\\$%&*\\(\\)_\\+=\\^:\\.,;]\", \" \", text)  # Preserve negation words, some punctuation\n",
    "  \n",
    "  # Lemmatization (preferred)\n",
    "  text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "  # Clean URLs\n",
    "  text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',text)\n",
    "\n",
    "  # Clean Emails\n",
    "  text = re.sub('@[^\\s]+', ' ', text)\n",
    "  \n",
    "  # Stop word removal (optional)\n",
    "  text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model = model, tokenizer = tokenizer, max_len = 200):\n",
    "  \n",
    "  # Preprocess the text\n",
    "  preprocessed_text = preprocess_text(text)  # Replace with your preprocessing function\n",
    "\n",
    "  # Tokenize the text\n",
    "  tokens = tokenizer.tokenize(preprocessed_text, padding='max_length', truncation=True)\n",
    "\n",
    "  # Convert tokens to IDs\n",
    "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "  # Pad the sequence (if model requires it)\n",
    "  padded_input = pad_sequences([input_ids], maxlen=max_len)\n",
    "\n",
    "  # Make the prediction\n",
    "  predicted_sentiment = model.predict(padded_input)[0][0]  # Assuming single output\n",
    "\n",
    "  # Define a threshold for sentiment classification (optional)\n",
    "  threshold = 0.5  # Adjust threshold based on your model's output range\n",
    "\n",
    "  sentiment_label = 'Positive' if predicted_sentiment > threshold else 'Negative'\n",
    "\n",
    "  return sentiment_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_csv(csv_file):\n",
    "      df = pd.read_csv(csv_file)\n",
    "      \n",
    "      # Check if there's a text column for prediction\n",
    "      if 'text' not in df.columns:\n",
    "          return \"CSV must contain a 'text' for prediction\"\n",
    "      \n",
    "      # Add predictions as a new column in the DataFrame\n",
    "      df['prediction'] = df['text'].apply(lambda text: predict_sentiment(text))\n",
    "      \n",
    "      # Return rendered template with results\n",
    "      return df['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nngpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
